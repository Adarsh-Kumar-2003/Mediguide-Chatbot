{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ftTdEAtjbLvC",
    "outputId": "2b01d5d4-6e9c-4e9f-989f-0a35d9b18a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.2\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "Successfully installed trl-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate peft trl safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HNLTjmDbeCO",
    "outputId": "3488e015-427a-4906-bf55-88544d835f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu126\n",
      "Transformers: 4.56.1\n",
      "CUDA: True, GPU: Tesla T4, Mem: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import transformers, datasets, accelerate, peft, trl\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0)}, Mem: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ao_q1Sxkbjhu"
   },
   "outputs": [],
   "source": [
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "dataset_name = \"keivalya/MedQuad-MedicalQnADataset\"\n",
    "new_model = \"DialoGPT-large-medical-chat\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    warmup_steps=100,\n",
    "    remove_unused_columns=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "referenced_widgets": [
      "444e0913e142478aae0329ad32688227",
      "57377b33688c47598925a9f248ef8c19",
      "18eaa5dc37bd430492d1123cb0ef608d",
      "3ce7bde23c3f4a91b2caf4e55ef89b5c",
      "0f12a06e3af24d088cf0572892b5beee",
      "394c8e4a2dc64493aca37bd9d5eac8b8",
      "ecf00b7583de496a863eabebaa424535",
      "60061b3ac8dc48658f8933155cd989ed",
      "964a6847c0e24ed49c4e67e427cbc1af",
      "016fe750871f4011b68b1c3b1bc00358",
      "128881689e644ee4a95a581660a222f8",
      "7f65473b1e6d4881aac6d3f285b974bc",
      "178d887669704e9bb8323b62ecf00f8d",
      "a0317b0d974b42d4b14a09960306fb3a",
      "5a3f6bd7fe7e4cc982ce2ab55ff4bc0f",
      "2360ebf02d534a3ea087d253fb182281",
      "922ce68bfd274eb6b6974fe925d0e844",
      "c1c9d6a6c0204b4b8374b0a348d61d4b",
      "11d68e8f4e1247dcb5831daf14b3b79a",
      "05f20e5f581c4659bdcfb461affd92c9",
      "4fba4b9e6d9b4968bbef7f7c09640389",
      "7efc5bb63cd34207bdac9c4012eb3098",
      "2ffa1b9cf2cb46548bd58213d12af59d",
      "f2bedf594d0242bb8d440ea50aa7c9c4",
      "95f1b37c0fd0499d9d4a1bb4a4026ab4",
      "ad149d24e98f4ba88091f4c1384b3c93",
      "2617026597a5402185bc6a5f11502dce",
      "0eb126838f994342945d484d5bbc35ba",
      "274df8396ba540b8a472f7d7e34e5377",
      "1e7047f621664286a195e00a125f163c",
      "0139031a06d14f2a9e653d107008be82",
      "5ab64e63be014f6788a6c24f38a038e0",
      "0e686a642fe34778a558a9a7e7defccb"
     ]
    },
    "id": "dxfqfOyybn6Z",
    "outputId": "24f399fa-5e47-4330-ab8d-8abb0fc25216"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444e0913e142478aae0329ad32688227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f65473b1e6d4881aac6d3f285b974bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "medDataset_processed.csv:   0%|          | 0.00/22.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffa1b9cf2cb46548bd58213d12af59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2000, Eval: 200\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\").shuffle(seed=42)\n",
    "train_dataset = dataset.select(range(2000))\n",
    "eval_dataset = dataset.select(range(2000, 2200))\n",
    "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330,
     "referenced_widgets": [
      "cb058f890c13433daf035ce4d12b7193",
      "464d896c60dd4511b4d6613f7e910d6e",
      "46f3cd426c0644b3ab27c76981b526f3",
      "22ce1ea496a94b23a2e959c2ef4cc0f0",
      "ecc6f599c7e64c73afc874539676800f",
      "3c0b430c1175480d82390ff82312f413",
      "806e6f966684458eab843e31ca7a8c09",
      "8e243730073b4658b6a56038bbd359c8",
      "79093bfb535c4859a8c6764c0aa4d171",
      "a2d227c7640b419d8c2e6073a8315924",
      "b37153f6f1ef4a8fbffa0f7a647f03e6",
      "840f013f39af492e898020c9f54b21ca",
      "827c0864046d446885b73aa89cbe69f8",
      "f704f4ab947d43af9de3e88bc884f766",
      "6a8b52842471406ca759414f747313f2",
      "4e8088864cde453d9776c707ad331e7d",
      "c62671bfe5dc496586c5b30ddad66fb7",
      "bce5407cdc154de5a2505b07330b00c2",
      "eb71663ee73140389751b653767cdb7d",
      "514613e930a74c2abbe109b1dd7c6094",
      "12ec49bc797040118b49b25e102b652d",
      "2b2813c6967642ebb2d5c18986591a11",
      "b637143163fe478f837597d28595dd68",
      "3d7101ef077147169df6de46b4354c9f",
      "a57854f3ebfb41efa0b42b99e9c8c0bc",
      "65682e0e02034aa09b62ffbf900a4079",
      "f21b7a37c57844fd8a2f45a69c883ff6",
      "1c91bc36b87f49489d36fc5c381c55de",
      "d219f9ac09d44584917fe5428bcf366e",
      "9bf8c84954ea440cb0e34e2d89e1ece3",
      "460f477fab0742eaa1916600ffce8bb0",
      "b7881b30fe7b4421b9631093d65b1517",
      "035f9f6419bd45a09e136cd41f22ab16",
      "9f0e549ce9ef43278a94584803db296e",
      "cee44b3aa3c94ef1a55210d18fa3aecf",
      "4f4dc65b3d0e4ee99a53d0feb48ac252",
      "8557d619907f4f1da6ea171718ec1b46",
      "92aee8eca1284a30b5ab944a3558c37e",
      "2b5445a3cd0f4bd8a61b16619318e237",
      "670c4c7f73064badb0824b756f828d2a",
      "71bf704496a746cdbd1392dca1cf6901",
      "419ccb6609494a7aa827e6664b5fabab",
      "cdef859721754dee9462b21f5103cc50",
      "b684f6624567466ea98849f9eb738ef8",
      "6b2680ab3ab542b08dc98384669afe82",
      "e3221a5371774a07ae9ea54d087483b5",
      "f74f790162a543e39bd45036ff666d54",
      "88342ed8eae348f292f7bc69f4589c50",
      "e78ec3c5e10349d29e11d70b1bb4a2e8",
      "0847c4a9e5554c28835e9b5ba632487b",
      "04862cf1649b43fbad63e57886d2fe9e",
      "53261aa798a84fdc88c39d738bab90f8",
      "a62223bb0abc44e4bb82cda884e37200",
      "902d93453d9043e8a4077915b977bcbc",
      "6c4f80833995456b8c0cc579b01af303",
      "b07ed61b2b0541e680cfae8be482e921",
      "3a4213c0f18a4b4082de504fd37e7c11",
      "11d053f6dd844997aca512bd02d400be",
      "33ed4410de9147d8b229c513599f4236",
      "22b66cb50df84b0293f123f8f22e3387",
      "597d2e458bb74455a67efd04ee4fdfb5",
      "67bea1b87d0243318be5944ad1d9c90e",
      "09318193cfd042b3a2a6dad7b5d25cdd",
      "96b3bfad0a0e4afdba58e581dc31ddd5",
      "75b3a634870d401baa526f310dba8b83",
      "45ec0191e41341f898aed11a03c8c8d0",
      "f20532dae3fc4dd3b9b8563f81171158",
      "6a7134c620274edf97c19352be76044e",
      "102f5c4393fe4a23b1ad24abe3a1bee0",
      "be781c0775244d9ca43adb3d4c78f737",
      "23547006c67a45b290288bc24b3df6e4",
      "4aa3709ba567466eb81cf093282fe3d2",
      "33647354698a400a9ec7894172bfd81a",
      "1227e063b9cf4776805dafd19e566200",
      "ab9e3889cc874f739102ac8531e006af",
      "97aa00acf70e4a6da3a2dc5b377d5a06",
      "d7e22ce1f2ba40879a53228e2abb0e76"
     ]
    },
    "id": "HK0mrUPjbrXQ",
    "outputId": "3c4e3d75-05d5-4b6b-8d02-db871417fde1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb058f890c13433daf035ce4d12b7193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840f013f39af492e898020c9f54b21ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b637143163fe478f837597d28595dd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0e549ce9ef43278a94584803db296e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2680ab3ab542b08dc98384669afe82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07ed61b2b0541e680cfae8be482e921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20532dae3fc4dd3b9b8563f81171158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 23,592,960 || all params: 797,623,040 || trainable%: 2.9579\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.train()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "edc123a98a4041ddbd9bde58ba243fa0",
      "cd217ca05e1541e29c75f0854b83b3c1",
      "00e246da0a5945268c0a31cfc38d4760",
      "fe9d57307bea47f0bf24dd77e38cfe97",
      "6f28b0e4f56946b187c448a39e5c0287",
      "7b4b4dd3ff584c7da17f9969928e4a3b",
      "68f042f728e742aeb59098bf0e54976b",
      "9954f462980f41549c2d30203c823859",
      "1e570413afe54a0ab6ea5ee9d1f04a9d",
      "f12e75096d57412e9171d74b3b7fd4cf",
      "1bb8957870f047bfb740b1ec486e943b",
      "49385b18870e4c398b49189a960bfae1",
      "a820d28159084f919e15017dc5346bd3",
      "d8d2685652984b14a64f2c01fab26690",
      "ee7997a220664b0a8968c95f1b8b45cf",
      "684afbf8280a41ba85af4e61a289aea3",
      "4ed596afe7ad40f090320ffb3c1a4b9b",
      "d7c64c81005b4b60ad4a00506c29ae65",
      "4dab141ab3eb472e9152d7508c5971eb",
      "37fdcd069d3e419d9e56a0339934e43c",
      "860deb11abcd4dc6a568db152194a614",
      "cf6b749775f5448e93e61c446d1579f2"
     ]
    },
    "id": "HdORyOPdbtRC",
    "outputId": "b22eb9f1-b6da-4c14-83a1-c9544256439e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc123a98a4041ddbd9bde58ba243fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49385b18870e4c398b49189a960bfae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_and_tokenize(examples):\n",
    "    if isinstance(examples['Question'], str):\n",
    "        texts = [f\"{examples['Question']}<|endoftext|>{examples['Answer']}<|endoftext|>\"]\n",
    "    else:\n",
    "        texts = [f\"{q}<|endoftext|>{a}<|endoftext|>\" for q, a in zip(examples['Question'], examples['Answer'])]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokenized[\"labels\"] = [ids.copy() if hasattr(ids, 'copy') else ids[:] for ids in tokenized[\"input_ids\"]]\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(format_and_tokenize, batched=True, batch_size=100, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(format_and_tokenize, batched=True, batch_size=100, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7RmqDSabvV8",
    "outputId": "8fa58fad-f775-4038-eba4-d6571396ee91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2800120471.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZP_PVQPcbx0S",
    "outputId": "aeea4f47-70f1-4473-98e7-2e4af1fec4c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkumataaloo\u001b[0m (\u001b[33mkumataaloo-indian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250924_063807-4y7n3zp4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kumataaloo-indian-institute-of-technology/huggingface/runs/4y7n3zp4' target=\"_blank\">iconic-pond-3</a></strong> to <a href='https://wandb.ai/kumataaloo-indian-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kumataaloo-indian-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/kumataaloo-indian-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kumataaloo-indian-institute-of-technology/huggingface/runs/4y7n3zp4' target=\"_blank\">https://wandb.ai/kumataaloo-indian-institute-of-technology/huggingface/runs/4y7n3zp4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 25:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>2.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>2.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>2.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>2.110900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.082300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>2.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.104100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Training on {len(train_dataset)} samples\")\n",
    "trainer.train()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4tADCKNbytw",
    "outputId": "b002c10b-27ae-4193-a072-87c24e33985b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: DialoGPT-large-medical-chat\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "print(f\"Model saved to: {new_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAJDk3n6bz6T",
    "outputId": "47967538-4210-4c45-b0c9-9b2a1dd2ad8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Question: What are the symptoms of diabetes?\n",
      "   Answer: What are the symptoms of diabetes?The following : Symptoms of Diabetes:  - Diabetes. The body is trying to tell you something about glucose absorption...\n",
      "\n",
      "2. Question: How is hypertension treated?\n",
      "   Answer: How is hypertension treated?There are several different treatment methods. These include:  - using a steroid-drug cocktail (pED) that reduces the risk...\n",
      "\n",
      "3. Question: What causes heart disease?\n",
      "   Answer: What causes heart disease?Heart failure may occur if a person has an illness in their blood. You can have any of two types of kidney problems:  - acut...\n",
      "\n",
      "4. Question: What are the side effects of aspirin?\n",
      "   Answer: What are the side effects of aspirin?I take it for pain and fatigue. The side affects include insomnia, weight gain, swelling in my thighs after a lon...\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "test_questions = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"How is hypertension treated?\",\n",
    "    \"What causes heart disease?\",\n",
    "    \"What are the side effects of aspirin?\",\n",
    "]\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    test_input = f\"{question}<|endoftext|>\"\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    parts = response.split(\"<|endoftext|>\")\n",
    "    answer = parts[1].strip() if len(parts) > 1 else response.replace(test_input, \"\").strip()\n",
    "    print(f\"   Answer: {answer[:150]}{'...' if len(answer) > 150 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "Nm6lBC1xb4x6",
    "outputId": "3459f5aa-5757-4ad6-c679-3a97b5ead053"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
       "            url.searchParams.set('tensorboardColab', 'true');\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqYdSeDwdSLo",
    "outputId": "40115f20-5904-43c6-9aaa-0eedfb668c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. What is diabetes and how is it treated?\n",
      "   Answer: Diabetes is a disease that affects the immune system and occurs when the body tries to fight its way through the body's digestive system. It is treatable by taking insulin. The blood test and the bloo...\n",
      "\n",
      "2. What are the symptoms of hypertension?\n",
      "   Answer: What are the signs and symptoms of hypertension? The Human Phenotype Ontology provides the following list of signs and symptoms for hypertension. If the information is available, the table below inclu...\n",
      "\n",
      "3. How does aspirin work as a medication?\n",
      "   Answer: aspirin is a type of aspirin, it is an antioxidant. aspirin inhibits the production of cytokines by making the blood vessels more dilated. The medication acts as a barrier between the blood vessels an...\n",
      "\n",
      "4. What causes heart disease?\n",
      "   Answer: What causes heart disease? The exact cause of heart disease varies widely in the family. However, it is believed to lead to a variety of conditions including diabetes, obesity, heart disease, liver di...\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is diabetes and how is it treated?\",\n",
    "    \"What are the symptoms of hypertension?\",\n",
    "    \"How does aspirin work as a medication?\",\n",
    "    \"What causes heart disease?\",\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=300,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i}. {prompt}\")\n",
    "    formatted_prompt = f\"{prompt}<|endoftext|>\"\n",
    "    result = pipe(formatted_prompt)[0]['generated_text']\n",
    "    answer = result.split(\"<|endoftext|>\")[1] if \"<|endoftext|>\" in result else result\n",
    "    print(f\"   Answer: {answer.strip()[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xm_81i0FdVA_"
   },
   "outputs": [],
   "source": [
    "del model, pipe, trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I__qXEXCdVz0",
    "outputId": "1e08c7c5-0981-4c75-fb81-9d015c2b4020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merged LoRA weights into base model.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "print(\" Merged LoRA weights into base model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBX97AWqdYAl",
    "outputId": "efccd233-3412-41ab-8d7a-29dfcdaa1b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenizer ready for upload.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import locale\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "print(\" Tokenizer ready for upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka0kc7pVddcO",
    "outputId": "879302b7-f172-4e90-b890-8402395f1fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenizer ready for upload.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import locale\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "print(\" Tokenizer ready for upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsupIsMCdeth",
    "outputId": "e21adf37-51bf-4741-d42c-35b662714544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: write).\n",
      "The token `AdarshKumarmedicalchatbotnew` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `AdarshKumarmedicalchatbotnew`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "93965d4266a14b69884b77aaaf67d279",
      "21e76ccf001242e2b3c1237d5cce2917",
      "7e1727c5b1f14ba3b7c7dbb7c1689014",
      "10f0e4e56dd44276aa1887ff8ee693cc",
      "dd62e53152f64833aa842a9a0a8f2ee1",
      "d77b2fd0f1524e0da509faf132a81b33",
      "4596d3e466d4452b9dad81a3ff9fc2b2",
      "b6c551dcbf924bd98a109ce7e5bd9d87",
      "bd0138cb81174f87b2fb373578e25599",
      "9e98d7fb13e24f3284e76cecad04763a",
      "f20de5a61519439884b00073bbe0d238",
      "7cd85b9c96064766b21432869d6f7bae",
      "bb1f5c054d4d4a28a8644fd8def67c77",
      "b8f24b9884684f4fa3769108087356d8",
      "8f15010f8c664ac8b93ec9f5567cc31a",
      "c68be98ad916433abcadffdf2004ac82",
      "560d05154b1d4cf3a4e77e21cf4b68c6",
      "11b7bd14fec940a783c9b835f6b0a6eb",
      "e0458cbeaa624f3b84fe4aece35d0649",
      "be08d1a1862f4e3f8860d1e62ddc48d6",
      "da0455ea73c14788aec91d3c9bf4c384",
      "da84fb1844484a66843df9d14576d90a",
      "7fc7399c706241c9800586736be14f51",
      "25778233bf084cbbb45a612cbb96f26c",
      "49fa5d8864384d4a9a4f77554c488ab2",
      "9c105d6961214377a3d50f72634db873",
      "cb44c82f7c544254affbc4d867f3e5d1",
      "1bd80a5e4dd540a59d45145b9c34de5b",
      "85254a07cc1644f593f4c3a30cae3445",
      "f1d2780debeb43c88f7a6e7a30e53e91",
      "ceeec66a87604e9cb481a39d70769955",
      "f9726b7f8d7d4ee1a6b38da7192c5188",
      "1bd1b2f5352147c7a1c863c9c6654da7"
     ]
    },
    "id": "-0qXfqcodfhM",
    "outputId": "c8440254-2239-403a-b9de-581d1460433e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93965d4266a14b69884b77aaaf67d279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd85b9c96064766b21432869d6f7bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc7399c706241c9800586736be14f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...al-chat/model.safetensors:   1%|1         | 16.7MB / 1.55GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uploaded to: https://huggingface.co/Adarsh123rv12/DialoGPT-large-medical-chat\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"Adarsh123rv12/DialoGPT-large-medical-chat\"  # CHANGE THIS!\n",
    "\n",
    "merged_model.push_to_hub(repo_name, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_name, use_auth_token=True)\n",
    "\n",
    "print(f\" Uploaded to: https://huggingface.co/{repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342,
     "referenced_widgets": [
      "ade17c1ac1cf4de3afb307738ad4c63b",
      "4673e505d7874b7e91ac2d1d269f3fa6",
      "ba2b7f4727454e5f99822ac11177515c",
      "369431c51a2f4679b7164d8e45b95f96",
      "a1d1c35a40084bdaa7c06fc83a02415a",
      "f45d314b4de347dabe3a4405996fe2bb",
      "477f2bde06234b60badee9e7bd688bf4",
      "be6dfac142364acda5ec13d69b4616e2",
      "5daecbdb188b4872ab068682865dd939",
      "058d7b5e3e704039a53b713241a8241a",
      "a8361e53dd9f4c88bfe6c740c0daf8a5",
      "862579dff4c646e8a9554538303d3aff",
      "1638ebb7b78946108d42ad255424eb72",
      "6bcde2d7880e4f2cbdc8b34287fad914",
      "1444fb508884426b996ad9385f6d2f99",
      "ff1e1b7fc01647f8b4d30067a1a4a24f",
      "2b17c61fd8d8439b95f60d602d339b0d",
      "0823fd1e694742b886f6339dc7de1cd5",
      "38e5bee6fb7144929ae0c653d61baa17",
      "f9107fe7c611408cacee84384d42bbd0",
      "923df700dcbc4bf996ef78fba9887241",
      "0435d3ce3330403381205bef369852af",
      "69b569c188484a53b16ca654416868bd",
      "36e2da589cbe4fdb8e7200541b305c1d",
      "5b3cd675b0d44adb8978c760a5563f9e",
      "caf74e67091b46d6bd4b5d0cdcb31e49",
      "47807178199b4766ae93ea68432da134",
      "4552dacd5bee425b9c8b62a5ca8332a4",
      "3b6f3d8cd8c74930b3f36a01d75d1102",
      "e239504c1e1e4b0b8361a6e11274545b",
      "4c55bf45bc2d462aa31e93823f4c121a",
      "cedeb8bf16f24aaba3fa85fff1991a4d",
      "1dd0cb4eebbe4526afd0a0d5707859c4",
      "e9d4da15f2e04c1c86f6c372ac2810bb",
      "ff9daff9b6534a90a20071ee299908e4",
      "56765c4d5cc348579dd4af7fcf85174b",
      "e8001362d52d49e8b0c84720adc5dc5f",
      "153f845d793546959e76c262a3717d1e",
      "25101fd96af0477688724fb02b643bc8",
      "901af443b5d24133845ddcbc4675aeff",
      "2d0e0a86412f45c8bd29145b032162ff",
      "23a847f249b049668f97504f9a6cfbb5",
      "21c51582ace54134b37c3ead5633734f",
      "b9d9249843d747df8c93a8c6e332308e",
      "e8cd19ab871a482a80a7deef5673e6bd",
      "02275227e2b44fe08bdebbeb132ebede",
      "4f600809e43c44cb81857fb8b965751b",
      "22a8808f72444514bcf80b8cc14e428e",
      "b0f91e4762604da1a0f5fc8337c2dd2e",
      "2d6cb56268ee493e96a5caa2e6c8b940",
      "88cf19f9cf78440b9472ca59946c41e3",
      "c317156a8a2146e2be61a35c8b347817",
      "f50f10dd485b42bebc4273aa5bf3d8bf",
      "fa74f8a0ca6e4be7b038da7436f19880",
      "3bcf2f706c744774bb0d418d31332cbe",
      "1fba532fdc784237b8bc1fb1be07519e",
      "0cbedcaee48441eba3ac359368b3dcfc",
      "442b2acb67034a30853adff9cbbc656f",
      "0ce387d07e7344a1ad35a5b433b58fae",
      "2172d478c37e4a098129367304382d52",
      "371be9376879440c88013e0c266e607b",
      "6ba13b2c507f49088faf8f986eed7033",
      "2f2bb3126ab6486487dba35af89200fd",
      "54e52ee0105d4257a2cdc25628fda40c",
      "45a6ae4ed3c543849e59437ab349629b",
      "a1f4b84a7c9a4966ba6b11fad8bf0af9",
      "c618ec99ab2e4880bc06c90ac43849d3",
      "f3bb23a81947460280ae0a7ab3a360d5",
      "b63460754fd64799899d9ce7adf42a18",
      "d9760dcce7834be98b198e5926e2b9c4",
      "33a79e0b700043cea08ff63eb9a4c3db",
      "d0ede2c3312b4abca82f8ea76c1010b0",
      "9737cf3ddd854da5a8a54a37b7a40784",
      "a22beb56993940eab0e3bee79c8a12e3",
      "707d3ebaa0f54a2a8351770f2a389e0e",
      "75420011b19b47728fe35c2956d95975",
      "5df05522b8734ada820523006a9656ad",
      "59d417c485964ef0bb9d8843ef5c3ce7",
      "249066dc8eca419b98a1c94d05715754",
      "a68f7dbbd4104ec5b9e3a829fb40c5c9",
      "21ee7e253cf543fbaf8d97e5bd5680d6",
      "d0f14f39f1ca45e38cb5142a3b7b07e9",
      "b5f584b7642f406792d7a6055ee06c68",
      "fdadd22d2faa4a5e820bc9b3ca30c600",
      "313f687a448041718a9c6bdba32fc4d8",
      "acd4a6f3d8044e289b75461bedddd5fb",
      "97620c0118cc4cc8999d717e821927fc",
      "b491c7ba7bb44960890127341426544f",
      "b394fbfd8d334d50800dfb8be157af74",
      "1e9ef0c7fdf54cefab8599a19dc5eabf",
      "b469c8387a6542198e4b3630e87defe0",
      "6df2ac5c012c4bea8132f3e33ca4b7cc",
      "e1cdb1dc53fb498da077bd81202f17ce",
      "2c3c493b48794bb891fb55e11af21292",
      "9a8082bf42e84dbb8404fae3d72d23fb",
      "b65b59f92b434685b0086c8900644693",
      "94ea14e86be54503af05a9c1ed0d2b1b",
      "7ae30fd552774af18cc4cc6a97bb401e",
      "5b48e4afb2ee4888972504de2c357eb2"
     ]
    },
    "id": "WhBgJCqDdhx4",
    "outputId": "243e8b6c-4f17-4533-af9c-9e40f75cc3fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade17c1ac1cf4de3afb307738ad4c63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/851 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862579dff4c646e8a9554538303d3aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b569c188484a53b16ca654416868bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d4da15f2e04c1c86f6c372ac2810bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cd19ab871a482a80a7deef5673e6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fba532fdc784237b8bc1fb1be07519e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c618ec99ab2e4880bc06c90ac43849d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d417c485964ef0bb9d8843ef5c3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/470 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b394fbfd8d334d50800dfb8be157af74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/77.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample output: What are the symptoms of diabetes?<|endoftext|>What are the signs and symptoms of diabetes? The Human Phenotype Ontology provides the following list of signs and symptoms for diabetic syndrome. If the information is available, the table below includes how often the symptom is seen in people with thi...\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "test_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=repo_name,\n",
    "    tokenizer=repo_name,\n",
    "    max_length=200\n",
    ")\n",
    "\n",
    "test_prompt = \"What are the symptoms of diabetes?<|endoftext|>\"\n",
    "result = test_pipe(test_prompt)[0]['generated_text']\n",
    "\n",
    "print(f\" Sample output: {result[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "cda9e2b694774644b611c530cea32c69",
      "ce9944dc7a124e02b63b1d64f540a8ba",
      "53e73fc9c98d4da996956a6271d2da69",
      "031901ca8ed54220a4bac0758e80dddf",
      "4388d145581f4cadbd56b355dd62255e",
      "da3ac52afc78415d9c5338ebc5729037",
      "2d0483d284e145b2b5790dffbf7786c2",
      "bb8c12c638664d148b84275dbe7616e7",
      "2f8550312fae4b79906868a5ac895394",
      "e65d5ba76aec4b68b5f1ca457bb0fb1f",
      "265b927e939f431399ab097c35d1551c"
     ]
    },
    "id": "3SiS7TBIdkAe",
    "outputId": "8a4a4b7d-7999-48e8-f385-502b6084ef65"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda9e2b694774644b611c530cea32c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [04:31<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation Results\n",
      "==============================\n",
      " Perplexity:      56.38\n",
      " ROUGE-L Score:   0.9893\n",
      " Avg Latency:     2.706 sec\n",
      " Model Size:      1647.34 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install evaluate rouge-score --quiet\n",
    "\n",
    "import torch, math, time, os\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "sample_eval = eval_dataset.select(range(100))\n",
    "\n",
    "references, predictions, latencies = [], [], []\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for sample in tqdm(sample_eval, desc=\"Evaluating\"):\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).cuda()\n",
    "    labels = sample[\"labels\"].unsqueeze(0).cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        loss = model(input_ids=input_ids, labels=labels).loss\n",
    "        output = model.generate(input_ids, max_new_tokens=50)\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "    pred_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ref_text = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred_text)\n",
    "    references.append(ref_text)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(predictions)\n",
    "perplexity = math.exp(avg_loss)\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "def get_model_size(path):\n",
    "    total_bytes = sum(\n",
    "        os.path.getsize(os.path.join(dp, f))\n",
    "        for dp, _, fn in os.walk(path) for f in fn\n",
    "    )\n",
    "    return total_bytes / 1e6\n",
    "\n",
    "model_size_mb = get_model_size(new_model)\n",
    "\n",
    "\n",
    "print(\"\\n Evaluation Results\")\n",
    "print(\"=\" * 30)\n",
    "print(f\" Perplexity:      {perplexity:.2f}\")\n",
    "print(f\" ROUGE-L Score:   {rouge_score['rougeL']:.4f}\")\n",
    "print(f\" Avg Latency:     {avg_latency:.3f} sec\")\n",
    "print(f\" Model Size:      {model_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSRvuNLHlkDx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
